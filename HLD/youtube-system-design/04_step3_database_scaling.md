# Step 3: Database Scaling (Replication & Sharding)

## The Problem We're Solving

Even with caching (Step 2), the database still has limitations:

```
Problems:
1. Single Point of Failure ğŸš¨
   - If database crashes, entire system goes down
   - No redundancy

2. Limited Read Capacity ğŸ“Š
   - Cache misses still hit database (10% of 87k req/sec = 8.7k req/sec)
   - Database can handle 10k queries/sec max
   - Close to capacity!

3. Limited Write Capacity âœï¸
   - 50 video uploads/sec (from Step 1)
   - Thousands of comments/sec
   - View count updates (even with batching)
   - Single database disk I/O becomes bottleneck

4. Backup & Disaster Recovery ğŸ’¾
   - No automated backups
   - Hardware failure = data loss
```

**Solution:** Database Replication + Sharding!

---

## Database Scaling Strategies

### Strategy 1: Vertical Scaling (Scale Up) - Temporary Fix

**What:** Upgrade to a bigger server

```
Before: 8 cores, 32 GB RAM, 1 TB SSD
After:  64 cores, 512 GB RAM, 10 TB SSD

Cost: $500/month â†’ $5,000/month
Capacity: 10k queries/sec â†’ 50k queries/sec
```

**Pros:**
- âœ… Simple (no code changes)
- âœ… Works for short-term scaling

**Cons:**
- âŒ Hard limits (largest server ~128 cores, 4 TB RAM)
- âŒ Expensive (exponential cost increase)
- âŒ Still a single point of failure
- âŒ Downtime during upgrades

**Verdict:** Good for startups, but YouTube needs horizontal scaling.

---

### Strategy 2: Horizontal Scaling - Replication (Scale Out) â­

**What:** Add multiple database servers

---

## Master-Slave Replication

### How It Works

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   WRITE PATH                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

App Server â†’ Master DB (writes only)
             â”‚
             â”œâ”€ INSERT new video
             â”œâ”€ UPDATE video title
             â””â”€ DELETE comment


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    READ PATH                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

App Server â†’ Slave DB 1 (reads only)
           â†’ Slave DB 2 (reads only)
           â†’ Slave DB 3 (reads only)
             â”‚
             â”œâ”€ SELECT video metadata
             â”œâ”€ SELECT user profile
             â””â”€ SELECT comments


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   REPLICATION                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Master DB â”€â”€(async copy)â”€â”€â†’ Slave DB 1
          â”€â”€(async copy)â”€â”€â†’ Slave DB 2
          â”€â”€(async copy)â”€â”€â†’ Slave DB 3
```

**Key Concept:**
- **Master:** Handles all writes (INSERT, UPDATE, DELETE)
- **Slaves (Replicas):** Handle all reads (SELECT)
- **Replication:** Master automatically copies data to slaves

---

## Architecture Diagram: Step 3

```mermaid
graph TB
    subgraph "Client Layer"
        Client[Users<br/>500M DAU]
    end

    subgraph "Load Balancing Layer"
        LB[Load Balancer<br/>Nginx]
    end

    subgraph "Application Layer"
        App1[App Server 1]
        App2[App Server 2]
        App3[App Server 3]
    end

    subgraph "Caching Layer"
        Redis[Redis Cluster<br/>5 nodes, 320 GB]
    end

    subgraph "Database Layer ğŸ†•"
        DBMaster[(Master DB<br/>MySQL/PostgreSQL<br/>WRITES ONLY<br/>- INSERT videos<br/>- UPDATE users<br/>- DELETE comments)]

        subgraph "Read Replicas"
            DBSlave1[(Slave DB 1<br/>READS ONLY<br/>US-East)]
            DBSlave2[(Slave DB 2<br/>READS ONLY<br/>US-West)]
            DBSlave3[(Slave DB 3<br/>READS ONLY<br/>EU)]
        end
    end

    subgraph "Storage"
        Storage[File Storage<br/>Local Disk]
    end

    Client --> LB
    LB --> App1
    LB --> App2
    LB --> App3

    App1 --> Redis
    App2 --> Redis
    App3 --> Redis

    App1 -->|Writes| DBMaster
    App2 -->|Writes| DBMaster
    App3 -->|Writes| DBMaster

    App1 -->|Reads| DBSlave1
    App1 -->|Reads| DBSlave2
    App1 -->|Reads| DBSlave3

    App2 -->|Reads| DBSlave1
    App2 -->|Reads| DBSlave2
    App2 -->|Reads| DBSlave3

    App3 -->|Reads| DBSlave1
    App3 -->|Reads| DBSlave2
    App3 -->|Reads| DBSlave3

    DBMaster -.Async Replication.-> DBSlave1
    DBMaster -.Async Replication.-> DBSlave2
    DBMaster -.Async Replication.-> DBSlave3

    App1 --> Storage
    App2 --> Storage
    App3 --> Storage

    style DBMaster fill:#ffccbc
    style DBSlave1 fill:#c8e6c9
    style DBSlave2 fill:#c8e6c9
    style DBSlave3 fill:#c8e6c9
    style Redis fill:#e1bee7
```

---

## What Changed from Step 2?

| Step 2 (Single DB) | Step 3 (Master-Slave) |
|--------------------|------------------------|
| 1 database server | âœ… 1 master + 3 slave servers (4 total) |
| All reads & writes to same DB | âœ… Writes to master, reads from slaves |
| Max 10k queries/sec | âœ… 40k queries/sec (4x capacity!) |
| Single point of failure | âœ… If slave fails, use other slaves |
| No geographic distribution | âœ… Slaves in US-East, US-West, EU |
| Backup = manual snapshots | âœ… Slaves are live backups |

---

## Replication: How Data Flows

### Example: User Uploads a Video

```
Step 1: Write to Master
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ App Server:                             â”‚
â”‚ INSERT INTO videos (id, title, ...)    â”‚
â”‚ VALUES ('abc123', 'My Vacation', ...)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
          â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Master DB:                              â”‚
â”‚ - Writes data to disk                   â”‚
â”‚ - Logs change to binlog (binary log)   â”‚
â”‚ - Returns success to app server         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


Step 2: Replicate to Slaves (Asynchronous)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Master DB:                              â”‚
â”‚ - Streams binlog to all slaves          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
          â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â†“             â†“             â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚Slave 1  â”‚   â”‚Slave 2  â”‚   â”‚Slave 3  â”‚
    â”‚US-East  â”‚   â”‚US-West  â”‚   â”‚EU       â”‚
    â”‚         â”‚   â”‚         â”‚   â”‚         â”‚
    â”‚Applies  â”‚   â”‚Applies  â”‚   â”‚Applies  â”‚
    â”‚changes  â”‚   â”‚changes  â”‚   â”‚changes  â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Step 3: Read from Slaves
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ User in New York watches video:        â”‚
â”‚ App routes to Slave 1 (US-East)        â”‚
â”‚ Low latency! (same region)              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Timing:**
- Write to master: ~50ms
- Replication lag to slaves: **100-500ms** (eventual consistency)
- Read from slave: ~50ms

---

## Replication Lag: The Consistency Problem

### Problem: Read-Your-Own-Writes

**Scenario:**
```
Time 0:00:00 - User uploads video "My Vacation"
              - App writes to Master DB âœ…

Time 0:00:00.1 - User refreshes page to see their video
                - App reads from Slave DB
                - Slave hasn't replicated yet (lag = 200ms)
                - User sees: "Video not found" âŒ

Time 0:00:00.3 - Replication completes
                - User refreshes again
                - Now sees "My Vacation" âœ…
```

**User experience:** "I just uploaded the video, why can't I see it?!" ğŸ˜ 

---

### Solution 1: Read-Your-Own-Writes (Simple)

```python
def upload_video(user_id, video_data):
    # Write to master
    master_db.execute("INSERT INTO videos (...) VALUES (...)")

    # Read from MASTER (not slave) for this user's next requests
    # Set a flag in user session
    session.set(f"force_master_read:{user_id}", True, ttl=10)  # 10 seconds

def get_user_videos(user_id):
    # Check if user recently wrote data
    if session.get(f"force_master_read:{user_id}"):
        return master_db.query("SELECT * FROM videos WHERE user_id = ?", user_id)
    else:
        # Normal users read from slave
        return slave_db.query("SELECT * FROM videos WHERE user_id = ?", user_id)
```

**Result:** User always sees their own writes immediately!

---

### Solution 2: Synchronous Replication (Slow but Consistent)

```python
# Wait for data to replicate to at least 1 slave before confirming write
master_db.execute("INSERT INTO videos (...) VALUES (...)", wait_for_replica=1)
```

**Trade-off:**
- âœ… Strong consistency (all reads see latest write)
- âŒ Slower writes (must wait for replication, +200ms latency)

**When to use:** Critical data (payment confirmations, account changes)

---

## Read/Write Splitting: Code Example

### Application Layer Logic

```python
import random

# Database connection pool
MASTER_DB = connect("mysql://master.youtube.com:3306")
SLAVE_DBS = [
    connect("mysql://slave1.youtube.com:3306"),  # US-East
    connect("mysql://slave2.youtube.com:3306"),  # US-West
    connect("mysql://slave3.youtube.com:3306"),  # EU
]

def execute_write(query, params):
    """All writes go to master"""
    return MASTER_DB.execute(query, params)

def execute_read(query, params):
    """Reads are load-balanced across slaves"""
    slave = random.choice(SLAVE_DBS)  # Simple round-robin
    return slave.execute(query, params)

# Usage
def upload_video(title, user_id):
    execute_write(
        "INSERT INTO videos (title, user_id) VALUES (?, ?)",
        (title, user_id)
    )

def get_video(video_id):
    return execute_read(
        "SELECT * FROM videos WHERE id = ?",
        (video_id,)
    )
```

---

## Failover: What Happens If Master Crashes?

### Problem: Master Failure

```
Time 0:00:00 - Master DB crashes (hardware failure) ğŸš¨
Time 0:00:00 - All writes fail (app returns errors)
Time 0:00:05 - Database admin notified
Time 0:00:30 - Admin promotes Slave 1 to new master
Time 0:01:00 - App config updated to point to new master
Time 0:01:05 - Writes resume âœ…

Downtime: ~1 minute
```

**Impact:** Users can't upload videos, like, or comment for 1 minute.

---

### Solution: Automatic Failover (High Availability)

Use tools like:
- **MySQL:** MySQL Router + Group Replication
- **PostgreSQL:** Patroni + etcd
- **Cloud:** AWS RDS Multi-AZ (automatic failover in ~30 seconds)

**How it works:**
```
1. Health check monitors master (every 1 second)
2. Master doesn't respond for 3 consecutive checks
3. Tool automatically promotes slave to master
4. DNS/load balancer updated
5. Apps reconnect to new master
6. Total downtime: < 30 seconds âœ…
```

---

## Database Sharding: Scaling Writes

### Problem: Master Still Bottlenecked for Writes

```
Even with replication:
- All WRITES still go to single master
- YouTube uploads: 50 videos/sec
- Comments: 1000/sec
- Likes: 5000/sec
- Total writes: ~6,000/sec

Single master limit: ~10,000 writes/sec
We're at 60% capacity (getting close!)
```

**Solution:** Shard the database (horizontal partitioning)

---

## What is Sharding?

**Simple Analogy:**

Imagine a library with 1 million books.

**No Sharding:**
- 1 librarian manages all 1M books
- Slow to find books (overwhelming!)

**With Sharding:**
- 4 librarians, each manages 250k books
- Librarian 1: A-F
- Librarian 2: G-M
- Librarian 3: N-S
- Librarian 4: T-Z
- 4x faster! (parallel processing)

---

## Sharding Strategy 1: Shard by User ID

### How It Works

```
hash(user_id) % num_shards = shard_id

Example:
user_id = 12345
hash(12345) % 4 = 1
â†’ Store in Shard 1

user_id = 67890
hash(67890) % 4 = 3
â†’ Store in Shard 3
```

### Shard Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              App Server                      â”‚
â”‚                                              â”‚
â”‚  user_id = 12345                             â”‚
â”‚  shard_id = hash(12345) % 4 = 1             â”‚
â”‚  route to Shard 1                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â†“            â†“            â†“           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Shard 0 â”‚  â”‚ Shard 1 â”‚  â”‚ Shard 2 â”‚  â”‚ Shard 3 â”‚
â”‚         â”‚  â”‚         â”‚  â”‚         â”‚  â”‚         â”‚
â”‚ Users:  â”‚  â”‚ Users:  â”‚  â”‚ Users:  â”‚  â”‚ Users:  â”‚
â”‚ 1, 5, 9 â”‚  â”‚ 2, 6, 10â”‚  â”‚ 3, 7, 11â”‚  â”‚ 4, 8, 12â”‚
â”‚         â”‚  â”‚         â”‚  â”‚         â”‚  â”‚         â”‚
â”‚ Videos: â”‚  â”‚ Videos: â”‚  â”‚ Videos: â”‚  â”‚ Videos: â”‚
â”‚ by theseâ”‚  â”‚ by theseâ”‚  â”‚ by theseâ”‚  â”‚ by theseâ”‚
â”‚ users   â”‚  â”‚ users   â”‚  â”‚ users   â”‚  â”‚ users   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### Pros & Cons: User ID Sharding

**Pros:**
- âœ… Even distribution (if user IDs are sequential)
- âœ… Simple to implement
- âœ… Easy to query user-specific data (all data in one shard)

**Cons:**
- âŒ Hard to query across users (e.g., "top 10 trending videos globally")
- âŒ Hot shard problem (if one user goes viral, their shard is overloaded)
- âŒ Can't easily re-shard (requires data migration)

---

## Sharding Strategy 2: Shard by Video ID

```
hash(video_id) % num_shards = shard_id

Example:
video_id = "abc123"
hash("abc123") % 4 = 2
â†’ Store in Shard 2
```

**Pros:**
- âœ… Distribute viral videos across shards (no hot shard)
- âœ… Easy to query video-specific data

**Cons:**
- âŒ Hard to query user-specific data (user's videos are scattered across shards)
- âŒ Complex joins (need to query multiple shards)

---

## Sharding Strategy 3: Shard by Geography

```
user_location = "US"
â†’ Store in US Shard

user_location = "EU"
â†’ Store in EU Shard
```

**Pros:**
- âœ… Low latency (data close to users)
- âœ… Regulatory compliance (GDPR: EU data must stay in EU)

**Cons:**
- âŒ Uneven distribution (US has more users than Iceland)
- âŒ Global queries are slow (must query all regions)

---

## Consistent Hashing: Solving the Re-Sharding Problem

### Problem: Adding Shards Requires Re-Hashing

```
Before (3 shards):
video_id = "abc123"
hash("abc123") % 3 = 2 â†’ Shard 2

After adding 4th shard (4 shards):
hash("abc123") % 4 = 3 â†’ Shard 3 (different shard!)

Result: Must migrate 75% of data! ğŸ˜±
```

---

### Solution: Consistent Hashing

**How it works:**
- Hash shards and keys onto a ring (0 to 2^32)
- Each key goes to the next shard clockwise

```
        0
        â”‚
   Shard A
        â”‚
    hash("abc123") = 100
        â”‚
    â†’ goes to Shard A
        â”‚
   Shard B (position 200)
        â”‚
   Shard C (position 300)
        â”‚
    2^32
```

**Adding a new shard:**
- Only keys between new shard and previous shard need migration
- ~25% of data moves (instead of 75%)

**Implementation:** Use libraries like `hash-ring` (Python) or `ketama` (C)

---

## Architecture Diagram: With Sharding

```mermaid
graph TB
    subgraph "Application Layer"
        App[App Server<br/>Shard Router]
    end

    subgraph "Shard 0 - US East"
        M0[(Master 0)]
        S0A[(Slave 0A)]
        S0B[(Slave 0B)]
        M0 -.Replicate.-> S0A
        M0 -.Replicate.-> S0B
    end

    subgraph "Shard 1 - US West"
        M1[(Master 1)]
        S1A[(Slave 1A)]
        S1B[(Slave 1B)]
        M1 -.Replicate.-> S1A
        M1 -.Replicate.-> S1B
    end

    subgraph "Shard 2 - EU"
        M2[(Master 2)]
        S2A[(Slave 2A)]
        S2B[(Slave 2B)]
        M2 -.Replicate.-> S2A
        M2 -.Replicate.-> S2B
    end

    subgraph "Shard 3 - Asia"
        M3[(Master 3)]
        S3A[(Slave 3A)]
        S3B[(Slave 3B)]
        M3 -.Replicate.-> S3A
        M3 -.Replicate.-> S3B
    end

    App -->|Writes for Shard 0| M0
    App -->|Writes for Shard 1| M1
    App -->|Writes for Shard 2| M2
    App -->|Writes for Shard 3| M3

    App -->|Reads| S0A
    App -->|Reads| S1A
    App -->|Reads| S2A
    App -->|Reads| S3A

    style M0 fill:#ffccbc
    style M1 fill:#ffccbc
    style M2 fill:#ffccbc
    style M3 fill:#ffccbc
    style S0A fill:#c8e6c9
    style S1A fill:#c8e6c9
    style S2A fill:#c8e6c9
    style S3A fill:#c8e6c9
```

---

## Capacity Calculation: How Many Shards?

### Given (from Step 1):
- Total writes: 6,000/sec (uploads, comments, likes)
- Single master capacity: 10,000 writes/sec

### Calculation:
```
Shards needed for writes = 6,000 / 10,000 = 0.6

Round up to 1 shard (for now)

But plan for future:
- Expected growth: 2x in 2 years
- Future writes: 12,000/sec
- Shards needed: 12,000 / 10,000 = 2 shards

Recommendation: Start with 4 shards (future-proof)
```

### Read Replicas per Shard:
```
Total reads (after cache): 8,700/sec (10% of 87k)
Reads per shard: 8,700 / 4 = 2,175/sec
Single slave capacity: 10,000 reads/sec

Slaves needed per shard: 2,175 / 10,000 = 0.22

Recommendation: 2 slaves per shard (for redundancy)
```

**Total database servers:**
```
4 shards Ã— (1 master + 2 slaves) = 12 servers
```

---

## Trade-offs: Replication vs Sharding

| Factor | Replication | Sharding |
|--------|-------------|----------|
| **Scales** | Reads (horizontal) | Writes (horizontal) |
| **Complexity** | Low | High |
| **Consistency** | Eventual (replication lag) | Complex (distributed transactions) |
| **Failover** | Automatic (promote slave) | Complex (shard unavailable) |
| **Queries** | Easy (single database) | Hard (cross-shard joins) |
| **When to use** | Read-heavy (YouTube!) | Write-heavy (Twitter, IoT) |

**YouTube's choice:** Replication first (read-heavy workload), sharding only if needed.

---

## Summary: Step 3 Achievements

### What We Added
âœ… Master-slave replication (1 master + 3 slaves per shard)
âœ… Read/write splitting (writes to master, reads from slaves)
âœ… Geographic distribution (US-East, US-West, EU)
âœ… Automatic failover (high availability)
âœ… Sharding strategy (4 shards, 12 total DB servers)

### Performance Improvements
âœ… Read capacity: 10k â†’ 40k queries/sec (4x)
âœ… Write capacity: 10k â†’ 40k writes/sec (4x with sharding)
âœ… Reduced latency: Geo-distributed slaves (closer to users)
âœ… High availability: No single point of failure

### Remaining Problems
âŒ Video transcoding is blocking (users wait for transcode to finish)
âŒ Storage doesn't scale (still on local disk)
âŒ No CDN (bandwidth costs are astronomical)
âŒ Monolithic app server (hard to scale individual features)

---

## What's Next?

In **Step 4**, we'll add **message queues** and **background workers** to handle:
- Asynchronous video transcoding (don't block user uploads)
- Email notifications (new subscribers, comments)
- Recommendation engine updates
- Analytics processing

Let's continue! ğŸš€
